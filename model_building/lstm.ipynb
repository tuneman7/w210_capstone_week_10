{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c844e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from fastai.tabular.all import *\n",
    "from fastcore.utils import *\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split as fuck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6406e15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-27 22:58:22.877166: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-27 22:58:23.143498: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-27 22:58:23.807853: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /programs/anaconda3/envs/data_engineering_venv/lib/\n",
      "2022-10-27 22:58:23.808012: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /programs/anaconda3/envs/data_engineering_venv/lib/\n",
      "2022-10-27 22:58:23.808019: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2022-10-27 22:58:24.407906: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-27 22:58:24.408191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-27 22:58:24.425814: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-27 22:58:24.426065: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-27 22:58:24.426280: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-27 22:58:24.426490: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "for device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51f0998e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1045753/1792947496.py:1: DtypeWarning: Columns (37,38) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  x = pd.read_csv(\n"
     ]
    }
   ],
   "source": [
    "x = pd.read_csv(\n",
    "        '../data_engineering/datasets/merged/us_cities_homes_sold_below_listing_and_price_cuts_rdc_data_w_macroecon.csv',\n",
    "        dtype={'month_day_yyymm': str, 'county_fip': str, 'quality_flag': str, 'unemployment_rate': float}, )\n",
    "x.drop(x.tail(1).index, inplace=True)\n",
    "x = x.replace('none', np.nan)\n",
    "# statepre = x['county_name'].str.split(', ', expand=True)[1]\n",
    "# x = pd.concat([x, statepre],axis=1)\n",
    "# x = x.rename(columns={1:'StatePre'})\n",
    "# x[\"population_county_annual\"] = x[\"population_county_annual\"].astype(str).astype(float)\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "x[\"median_listing_price\"].interpolate(method='linear',inplace=True,limit_direction='both' )\n",
    "x[\"unemployment_rate\"].interpolate(method='linear',inplace=True, limit_direction='both')\n",
    "x[\"active_listing_count\"].interpolate(method='linear',inplace=True, limit_direction='both')\n",
    "x[\"median_days_on_market\"].interpolate(method='linear',inplace=True, limit_direction='both')\n",
    "# x[\"population_county_annual\"] = x[[\"population_county_annual\", \"StatePre\"]].groupby(\"StatePre\").transform(lambda x: x.fillna(x.mean()))\n",
    "x[\"30yr_interest_rate\"].interpolate(method='linear',inplace=True, limit_direction='both')\n",
    "# x[\"inflation_rate\"].interpolate(method='linear',inplace=True)\n",
    "# x[\"federal_fund_rate\"].interpolate(method='linear',inplace=True)\n",
    "# x[\"Quarterly_GDP_State_Chained_2012\"] = x[[\"Quarterly_GDP_State_Chained_2012\", \"StatePre\"]].groupby(\"StatePre\").transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "d2 = x[['Date', 'county_name','median_listing_price','unemployment_rate','active_listing_count','median_days_on_market', '30yr_interest_rate']]\n",
    "d3 = d2.groupby('county_name').filter(lambda x : len(x)==74)\n",
    "d3['Date'] =  pd.to_datetime(d3['Date'], format='%Y-%m')\n",
    "\n",
    "d3 = d3.set_index('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed8c6d74-21ba-4aeb-bbd3-45821be04ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>county_name</th>\n",
       "      <th>median_listing_price</th>\n",
       "      <th>unemployment_rate</th>\n",
       "      <th>active_listing_count</th>\n",
       "      <th>median_days_on_market</th>\n",
       "      <th>30yr_interest_rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-08-01</th>\n",
       "      <td>zavala, tx</td>\n",
       "      <td>139500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>5.2225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-01</th>\n",
       "      <td>zavala, tx</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>10.7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>5.4125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-01</th>\n",
       "      <td>zavala, tx</td>\n",
       "      <td>129000.0</td>\n",
       "      <td>10.8</td>\n",
       "      <td>6.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>5.5220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-01</th>\n",
       "      <td>zavala, tx</td>\n",
       "      <td>129000.0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>5.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>5.2300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-01</th>\n",
       "      <td>zavala, tx</td>\n",
       "      <td>129000.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>4.9825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-01</th>\n",
       "      <td>zavala, tx</td>\n",
       "      <td>170000.0</td>\n",
       "      <td>12.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>3.7700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-01</th>\n",
       "      <td>zavala, tx</td>\n",
       "      <td>170000.0</td>\n",
       "      <td>14.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>3.4700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-09-01</th>\n",
       "      <td>zavala, tx</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>15.9</td>\n",
       "      <td>6.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>3.4600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-01</th>\n",
       "      <td>zavala, tx</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>16.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>3.4350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-07-01</th>\n",
       "      <td>zavala, tx</td>\n",
       "      <td>150000.0</td>\n",
       "      <td>16.8</td>\n",
       "      <td>6.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>3.4400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           county_name  median_listing_price  unemployment_rate  \\\n",
       "Date                                                              \n",
       "2022-08-01  zavala, tx              139500.0                0.0   \n",
       "2022-07-01  zavala, tx              150000.0               10.7   \n",
       "2022-06-01  zavala, tx              129000.0               10.8   \n",
       "2022-05-01  zavala, tx              129000.0                9.7   \n",
       "2022-04-01  zavala, tx              129000.0                9.5   \n",
       "...                ...                   ...                ...   \n",
       "2016-11-01  zavala, tx              170000.0               12.6   \n",
       "2016-10-01  zavala, tx              170000.0               14.1   \n",
       "2016-09-01  zavala, tx              150000.0               15.9   \n",
       "2016-08-01  zavala, tx              150000.0               16.5   \n",
       "2016-07-01  zavala, tx              150000.0               16.8   \n",
       "\n",
       "            active_listing_count  median_days_on_market  30yr_interest_rate  \n",
       "Date                                                                         \n",
       "2022-08-01                   4.0                  105.0              5.2225  \n",
       "2022-07-01                   4.0                   79.0              5.4125  \n",
       "2022-06-01                   6.0                   74.0              5.5220  \n",
       "2022-05-01                   5.0                   46.0              5.2300  \n",
       "2022-04-01                   4.0                  155.0              4.9825  \n",
       "...                          ...                    ...                 ...  \n",
       "2016-11-01                   3.0                  132.0              3.7700  \n",
       "2016-10-01                   3.0                  114.0              3.4700  \n",
       "2016-09-01                   6.0                   80.0              3.4600  \n",
       "2016-08-01                   5.0                   52.0              3.4350  \n",
       "2016-07-01                   6.0                   59.0              3.4400  \n",
       "\n",
       "[74 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d3[d3[\"county_name\"]==\"zavala, tx\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38681474-7b6a-4805-bc8e-49a09e344f18",
   "metadata": {},
   "source": [
    "# PREPROCESS BARK BARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b013340-5d6e-403a-9e35-9f1f0933dee4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocess dat\n",
    "# Because this is panel data so I will split each county data \n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler\n",
    "\n",
    "class DWrapper:\n",
    "    def __init__(self, xtrain,xtest,ytrain,ytest):\n",
    "        self.x_train = xtrain\n",
    "        self.x_test = xtest\n",
    "        self.y_train = ytrain\n",
    "        self.y_test = ytest\n",
    "        \n",
    "def my_train_test_split(data, yvar):\n",
    "\n",
    "    size=int(len(data)*0.90)\n",
    "\n",
    "    x_train =data.drop(columns=[f'{yvar}']).iloc[0:size] \n",
    "    x_test = data.drop(columns=[f'{yvar}']).iloc[size:]\n",
    "    \n",
    "    y_train=data[f'{yvar}'].iloc[0:size] \n",
    "    y_test=data[f'{yvar}'].iloc[size:] \n",
    "    \n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "\n",
    "\n",
    "def clean_splitted(d):\n",
    "    scaler = StandardScaler()\n",
    "    county=list(set(d.county_name))\n",
    "    X_train=[]\n",
    "    X_test=[]\n",
    "    Y_train=[]\n",
    "    Y_test=[]\n",
    "    global c\n",
    "    \n",
    "    # scalar numeric\n",
    "\n",
    "    cols = list(d.columns)\n",
    "    cols.remove('county_name')\n",
    "    # cols.remove('Date')\n",
    "\n",
    "    print('Scalar fit transforming to standardscaler')\n",
    "    d[cols] = scaler.fit_transform(d[cols])\n",
    "    \n",
    "    for i in range(0,len(county)):\n",
    "        data=d[d['county_name']==county[i]]\n",
    "        data.sort_values(inplace=True, by='Date', ascending=True)\n",
    "        c = county[i]\n",
    "        if i%200 == 0:\n",
    "            print(c)\n",
    "        # apply train_test_split for each county\n",
    "        if data.shape[0] != data[\"median_listing_price\"].shape[0]:\n",
    "            print(c)\n",
    "            print(data)\n",
    "        \n",
    "        x_train, x_test, y_train, y_test=my_train_test_split(data, 'median_listing_price')\n",
    "        X_train.append(x_train)\n",
    "        X_test.append(x_test)\n",
    "        Y_train.append(y_train)\n",
    "        Y_test.append(y_test)\n",
    "    \n",
    "    # concatenate test and trains from lists by counties\n",
    "    X_train=pd.concat(X_train)\n",
    "    Y_train=pd.DataFrame(pd.concat(Y_train))\n",
    "    # concatenate each test dataset in X_test list and Y_test list respectively\n",
    "    X_test=pd.concat(X_test)\n",
    "    Y_test=pd.DataFrame(pd.concat(Y_test))\n",
    "    \n",
    "    # one hot encode each split\n",
    "    print(\"one hot xtrain\")\n",
    "    X_train=pd.get_dummies(X_train,columns=['county_name'])\n",
    "    print(\"one hot xtest\")\n",
    "    X_test=pd.get_dummies(X_test,columns=['county_name'])\n",
    "    \n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "\n",
    "\n",
    "def scale_reshape(X_train, Y_train, X_test, Y_test):\n",
    "    track = [X_train, X_test, Y_train, Y_test]\n",
    "    \n",
    "    train = X_train.values\n",
    "    test = X_test.values\n",
    "\n",
    "    x_train = train.reshape((train.shape[0], 1, train.shape[1]))\n",
    "    x_test = test.reshape((test.shape[0], 1, test.shape[1]))\n",
    "    \n",
    "    # call reshape_data for final splits\n",
    "    print(\"Call wrapper reshape\")\n",
    "    return DWrapper(x_train, x_test, Y_train, Y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a8f435-22fb-42a1-a4ee-78794e1e7124",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a35454-41ab-4a64-8847-41396f080e8a",
   "metadata": {},
   "source": [
    "############## ONE HOT ENCODE ########################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c735ea3-a1f6-430b-ba87-2ca116feed54",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scalar fit transforming to standardscaler\n",
      "cook, il\n",
      "georgetown, sc\n",
      "mahnomen, mn\n",
      "cambria, pa\n",
      "greene, in\n",
      "st. john the baptist, la\n",
      "parmer, tx\n",
      "st. joseph, in\n",
      "barbour, wv\n",
      "warren, ms\n",
      "caribou, id\n",
      "butler, al\n",
      "haskell, ks\n",
      "baxter, ar\n",
      "perry, pa\n",
      "one hot xtrain\n",
      "one hot xtest\n"
     ]
    }
   ],
   "source": [
    "# Cleaned split and encoded of full dataframe\n",
    "X_train, Y_train, X_test, Y_test = clean_splitted(d3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72064456-6c94-4703-be4e-442172afc8fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2937"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(X_train.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19d2afe0-41e2-4442-9cdf-8025a027f056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call wrapper reshape\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "reshaped_data = scale_reshape(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dfcd30d-53ac-4a95-b0b0-3de770fed84a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((193578, 1), (193578, 1, 2937))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped_data.y_train.shape, reshaped_data.x_train.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "503d7f89-da61-48b8-bd9a-8c9832af1a8e",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: center;\">\n",
       "      <th></th>\n",
       "      <th>county_name</th>\n",
       "      <th>median_listing_price</th>\n",
       "      <th>unemployment_rate</th>\n",
       "      <th>active_listing_count</th>\n",
       "      <th>median_days_on_market</th>\n",
       "      <th>30yr_interest_rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-08-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>0.375</td>\n",
       "      <td>-1.654</td>\n",
       "      <td>-0.278</td>\n",
       "      <td>-0.743</td>\n",
       "      <td>2.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>0.543</td>\n",
       "      <td>0.428</td>\n",
       "      <td>-0.274</td>\n",
       "      <td>-1.079</td>\n",
       "      <td>2.288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-06-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>0.639</td>\n",
       "      <td>0.580</td>\n",
       "      <td>-0.286</td>\n",
       "      <td>-1.158</td>\n",
       "      <td>2.445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-05-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>0.545</td>\n",
       "      <td>0.428</td>\n",
       "      <td>-0.311</td>\n",
       "      <td>-1.178</td>\n",
       "      <td>2.028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>0.505</td>\n",
       "      <td>0.693</td>\n",
       "      <td>-0.325</td>\n",
       "      <td>-0.565</td>\n",
       "      <td>1.674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-03-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>0.870</td>\n",
       "      <td>0.958</td>\n",
       "      <td>-0.334</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-02-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>0.329</td>\n",
       "      <td>1.450</td>\n",
       "      <td>-0.330</td>\n",
       "      <td>0.305</td>\n",
       "      <td>-0.067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>0.272</td>\n",
       "      <td>1.412</td>\n",
       "      <td>-0.322</td>\n",
       "      <td>0.048</td>\n",
       "      <td>-0.521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.807</td>\n",
       "      <td>-0.317</td>\n",
       "      <td>-0.446</td>\n",
       "      <td>-1.016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.352</td>\n",
       "      <td>-0.312</td>\n",
       "      <td>-0.526</td>\n",
       "      <td>-1.060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>0.639</td>\n",
       "      <td>0.239</td>\n",
       "      <td>-0.304</td>\n",
       "      <td>-0.743</td>\n",
       "      <td>-1.060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.428</td>\n",
       "      <td>-0.306</td>\n",
       "      <td>-0.822</td>\n",
       "      <td>-1.299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.617</td>\n",
       "      <td>-0.305</td>\n",
       "      <td>-1.040</td>\n",
       "      <td>-1.381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-07-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>0.352</td>\n",
       "      <td>1.072</td>\n",
       "      <td>-0.306</td>\n",
       "      <td>-0.980</td>\n",
       "      <td>-1.345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>0.600</td>\n",
       "      <td>1.147</td>\n",
       "      <td>-0.313</td>\n",
       "      <td>-1.079</td>\n",
       "      <td>-1.192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>1.110</td>\n",
       "      <td>-0.319</td>\n",
       "      <td>-1.079</td>\n",
       "      <td>-1.210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>1.299</td>\n",
       "      <td>-0.331</td>\n",
       "      <td>-0.783</td>\n",
       "      <td>-1.070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>-0.041</td>\n",
       "      <td>1.829</td>\n",
       "      <td>-0.330</td>\n",
       "      <td>-0.486</td>\n",
       "      <td>-1.038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>0.101</td>\n",
       "      <td>2.018</td>\n",
       "      <td>-0.330</td>\n",
       "      <td>0.206</td>\n",
       "      <td>-1.427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>0.244</td>\n",
       "      <td>2.283</td>\n",
       "      <td>-0.317</td>\n",
       "      <td>0.522</td>\n",
       "      <td>-1.534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>0.163</td>\n",
       "      <td>1.715</td>\n",
       "      <td>-0.311</td>\n",
       "      <td>0.068</td>\n",
       "      <td>-1.607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>0.295</td>\n",
       "      <td>0.882</td>\n",
       "      <td>-0.309</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-1.492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.693</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>-1.393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>0.200</td>\n",
       "      <td>1.564</td>\n",
       "      <td>-0.274</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>-1.313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>0.300</td>\n",
       "      <td>1.867</td>\n",
       "      <td>-0.273</td>\n",
       "      <td>-0.170</td>\n",
       "      <td>-1.249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>0.286</td>\n",
       "      <td>3.078</td>\n",
       "      <td>-0.276</td>\n",
       "      <td>0.048</td>\n",
       "      <td>-1.133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>0.186</td>\n",
       "      <td>3.911</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>0.127</td>\n",
       "      <td>-0.924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>0.124</td>\n",
       "      <td>6.031</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>0.167</td>\n",
       "      <td>-0.824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>0.066</td>\n",
       "      <td>9.249</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>0.048</td>\n",
       "      <td>-0.719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>0.066</td>\n",
       "      <td>1.488</td>\n",
       "      <td>-0.267</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>-0.514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>0.072</td>\n",
       "      <td>1.034</td>\n",
       "      <td>-0.273</td>\n",
       "      <td>0.997</td>\n",
       "      <td>-0.492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>0.072</td>\n",
       "      <td>1.223</td>\n",
       "      <td>-0.252</td>\n",
       "      <td>1.056</td>\n",
       "      <td>-0.265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.617</td>\n",
       "      <td>-0.233</td>\n",
       "      <td>0.819</td>\n",
       "      <td>-0.128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.087</td>\n",
       "      <td>-0.216</td>\n",
       "      <td>0.344</td>\n",
       "      <td>-0.164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>-0.147</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>-0.159</td>\n",
       "      <td>-0.071</td>\n",
       "      <td>-0.174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>-0.111</td>\n",
       "      <td>-0.308</td>\n",
       "      <td>-0.292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>-0.237</td>\n",
       "      <td>0.087</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>-0.446</td>\n",
       "      <td>-0.277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>0.428</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.703</td>\n",
       "      <td>-0.064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>0.315</td>\n",
       "      <td>-0.155</td>\n",
       "      <td>-0.862</td>\n",
       "      <td>-0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>0.163</td>\n",
       "      <td>-0.195</td>\n",
       "      <td>-0.605</td>\n",
       "      <td>0.374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>-0.240</td>\n",
       "      <td>0.731</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>0.503</td>\n",
       "      <td>0.475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>-0.285</td>\n",
       "      <td>1.375</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>1.214</td>\n",
       "      <td>0.650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>-0.326</td>\n",
       "      <td>1.375</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>-0.326</td>\n",
       "      <td>1.412</td>\n",
       "      <td>-0.202</td>\n",
       "      <td>0.799</td>\n",
       "      <td>0.934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>-0.326</td>\n",
       "      <td>0.693</td>\n",
       "      <td>-0.185</td>\n",
       "      <td>0.522</td>\n",
       "      <td>1.182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-11-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>-0.326</td>\n",
       "      <td>0.087</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>0.463</td>\n",
       "      <td>1.508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>-0.340</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>-0.134</td>\n",
       "      <td>0.265</td>\n",
       "      <td>1.457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>-0.338</td>\n",
       "      <td>-0.140</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>-0.091</td>\n",
       "      <td>1.168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>-0.326</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>-0.348</td>\n",
       "      <td>1.057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>-0.343</td>\n",
       "      <td>0.352</td>\n",
       "      <td>-0.123</td>\n",
       "      <td>-0.624</td>\n",
       "      <td>1.025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-06-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>-0.343</td>\n",
       "      <td>0.277</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>-0.842</td>\n",
       "      <td>1.085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>-0.355</td>\n",
       "      <td>0.277</td>\n",
       "      <td>-0.189</td>\n",
       "      <td>-0.644</td>\n",
       "      <td>1.108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-04-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>-0.383</td>\n",
       "      <td>1.034</td>\n",
       "      <td>-0.214</td>\n",
       "      <td>-0.249</td>\n",
       "      <td>0.939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>-0.355</td>\n",
       "      <td>1.526</td>\n",
       "      <td>-0.223</td>\n",
       "      <td>0.681</td>\n",
       "      <td>0.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>-0.326</td>\n",
       "      <td>1.715</td>\n",
       "      <td>-0.236</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>-0.326</td>\n",
       "      <td>1.564</td>\n",
       "      <td>-0.235</td>\n",
       "      <td>0.938</td>\n",
       "      <td>0.318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-12-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>-0.326</td>\n",
       "      <td>0.996</td>\n",
       "      <td>-0.225</td>\n",
       "      <td>1.234</td>\n",
       "      <td>0.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>-0.371</td>\n",
       "      <td>0.466</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>0.938</td>\n",
       "      <td>0.160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>-0.412</td>\n",
       "      <td>0.201</td>\n",
       "      <td>-0.165</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>-0.446</td>\n",
       "      <td>0.428</td>\n",
       "      <td>-0.139</td>\n",
       "      <td>0.424</td>\n",
       "      <td>-0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>-0.446</td>\n",
       "      <td>0.504</td>\n",
       "      <td>-0.135</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-07-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>-0.474</td>\n",
       "      <td>0.845</td>\n",
       "      <td>-0.112</td>\n",
       "      <td>-0.249</td>\n",
       "      <td>0.225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-06-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>-0.474</td>\n",
       "      <td>0.580</td>\n",
       "      <td>-0.099</td>\n",
       "      <td>-0.526</td>\n",
       "      <td>0.135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>-0.497</td>\n",
       "      <td>0.617</td>\n",
       "      <td>-0.118</td>\n",
       "      <td>-0.545</td>\n",
       "      <td>0.286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-04-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>-0.497</td>\n",
       "      <td>1.110</td>\n",
       "      <td>-0.145</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-03-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>-0.497</td>\n",
       "      <td>1.640</td>\n",
       "      <td>-0.160</td>\n",
       "      <td>1.096</td>\n",
       "      <td>0.551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-02-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>-0.497</td>\n",
       "      <td>1.942</td>\n",
       "      <td>-0.173</td>\n",
       "      <td>1.175</td>\n",
       "      <td>0.511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>-0.495</td>\n",
       "      <td>1.867</td>\n",
       "      <td>-0.171</td>\n",
       "      <td>1.096</td>\n",
       "      <td>0.486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>-0.495</td>\n",
       "      <td>1.147</td>\n",
       "      <td>-0.132</td>\n",
       "      <td>0.938</td>\n",
       "      <td>0.554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-11-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>-0.467</td>\n",
       "      <td>0.617</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>0.938</td>\n",
       "      <td>-0.057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-10-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>-0.440</td>\n",
       "      <td>0.504</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>0.681</td>\n",
       "      <td>-0.485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-09-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>-0.440</td>\n",
       "      <td>0.542</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.384</td>\n",
       "      <td>-0.499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-08-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>-0.440</td>\n",
       "      <td>0.655</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.167</td>\n",
       "      <td>-0.535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-07-01</th>\n",
       "      <td>kalkaska, mi</td>\n",
       "      <td>-0.439</td>\n",
       "      <td>0.920</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>-0.528</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            county_name   median_listing_price  unemployment_rate  active_listing_count  median_days_on_market  30yr_interest_rate\n",
       "Date                                                                                                                              \n",
       "2022-08-01  kalkaska, mi          0.375              -1.654               -0.278                -0.743                 2.017      \n",
       "2022-07-01  kalkaska, mi          0.543               0.428               -0.274                -1.079                 2.288      \n",
       "2022-06-01  kalkaska, mi          0.639               0.580               -0.286                -1.158                 2.445      \n",
       "2022-05-01  kalkaska, mi          0.545               0.428               -0.311                -1.178                 2.028      \n",
       "2022-04-01  kalkaska, mi          0.505               0.693               -0.325                -0.565                 1.674      \n",
       "2022-03-01  kalkaska, mi          0.870               0.958               -0.334                 0.107                 0.517      \n",
       "2022-02-01  kalkaska, mi          0.329               1.450               -0.330                 0.305                -0.067      \n",
       "2022-01-01  kalkaska, mi          0.272               1.412               -0.322                 0.048                -0.521      \n",
       "2021-12-01  kalkaska, mi          0.172               0.807               -0.317                -0.446                -1.016      \n",
       "2021-11-01  kalkaska, mi          0.431               0.352               -0.312                -0.526                -1.060      \n",
       "2021-10-01  kalkaska, mi          0.639               0.239               -0.304                -0.743                -1.060      \n",
       "2021-09-01  kalkaska, mi          0.186               0.428               -0.306                -0.822                -1.299      \n",
       "2021-08-01  kalkaska, mi          0.349               0.617               -0.305                -1.040                -1.381      \n",
       "2021-07-01  kalkaska, mi          0.352               1.072               -0.306                -0.980                -1.345      \n",
       "2021-06-01  kalkaska, mi          0.600               1.147               -0.313                -1.079                -1.192      \n",
       "2021-05-01  kalkaska, mi         -0.012               1.110               -0.319                -1.079                -1.210      \n",
       "2021-04-01  kalkaska, mi         -0.027               1.299               -0.331                -0.783                -1.070      \n",
       "2021-03-01  kalkaska, mi         -0.041               1.829               -0.330                -0.486                -1.038      \n",
       "2021-02-01  kalkaska, mi          0.101               2.018               -0.330                 0.206                -1.427      \n",
       "2021-01-01  kalkaska, mi          0.244               2.283               -0.317                 0.522                -1.534      \n",
       "2020-12-01  kalkaska, mi          0.163               1.715               -0.311                 0.068                -1.607      \n",
       "2020-11-01  kalkaska, mi          0.295               0.882               -0.309                -0.011                -1.492      \n",
       "2020-10-01  kalkaska, mi          0.232               0.693               -0.291                -0.110                -1.393      \n",
       "2020-09-01  kalkaska, mi          0.200               1.564               -0.274                -0.091                -1.313      \n",
       "2020-08-01  kalkaska, mi          0.300               1.867               -0.273                -0.170                -1.249      \n",
       "2020-07-01  kalkaska, mi          0.286               3.078               -0.276                 0.048                -1.133      \n",
       "2020-06-01  kalkaska, mi          0.186               3.911               -0.256                 0.127                -0.924      \n",
       "2020-05-01  kalkaska, mi          0.124               6.031               -0.252                 0.167                -0.824      \n",
       "2020-04-01  kalkaska, mi          0.066               9.249               -0.256                 0.048                -0.719      \n",
       "2020-03-01  kalkaska, mi          0.066               1.488               -0.267                -0.051                -0.514      \n",
       "2020-02-01  kalkaska, mi          0.072               1.034               -0.273                 0.997                -0.492      \n",
       "2020-01-01  kalkaska, mi          0.072               1.223               -0.252                 1.056                -0.265      \n",
       "2019-12-01  kalkaska, mi          0.064               0.617               -0.233                 0.819                -0.128      \n",
       "2019-11-01  kalkaska, mi          0.015               0.087               -0.216                 0.344                -0.164      \n",
       "2019-10-01  kalkaska, mi         -0.147              -0.215               -0.159                -0.071                -0.174      \n",
       "2019-09-01  kalkaska, mi         -0.239              -0.140               -0.111                -0.308                -0.292      \n",
       "2019-08-01  kalkaska, mi         -0.237               0.087               -0.120                -0.446                -0.277      \n",
       "2019-07-01  kalkaska, mi         -0.212               0.428               -0.125                -0.703                -0.064      \n",
       "2019-06-01  kalkaska, mi         -0.212               0.315               -0.155                -0.862                -0.010      \n",
       "2019-05-01  kalkaska, mi         -0.113               0.163               -0.195                -0.605                 0.374      \n",
       "2019-04-01  kalkaska, mi         -0.240               0.731               -0.210                 0.503                 0.475      \n",
       "2019-03-01  kalkaska, mi         -0.285               1.375               -0.209                 1.214                 0.650      \n",
       "2019-02-01  kalkaska, mi         -0.326               1.375               -0.209                 0.997                 0.800      \n",
       "2019-01-01  kalkaska, mi         -0.326               1.412               -0.202                 0.799                 0.934      \n",
       "2018-12-01  kalkaska, mi         -0.326               0.693               -0.185                 0.522                 1.182      \n",
       "2018-11-01  kalkaska, mi         -0.326               0.087               -0.157                 0.463                 1.508      \n",
       "2018-10-01  kalkaska, mi         -0.340              -0.140               -0.134                 0.265                 1.457      \n",
       "2018-09-01  kalkaska, mi         -0.338              -0.140               -0.118                -0.091                 1.168      \n",
       "2018-08-01  kalkaska, mi         -0.326              -0.064               -0.118                -0.348                 1.057      \n",
       "2018-07-01  kalkaska, mi         -0.343               0.352               -0.123                -0.624                 1.025      \n",
       "2018-06-01  kalkaska, mi         -0.343               0.277               -0.139                -0.842                 1.085      \n",
       "2018-05-01  kalkaska, mi         -0.355               0.277               -0.189                -0.644                 1.108      \n",
       "2018-04-01  kalkaska, mi         -0.383               1.034               -0.214                -0.249                 0.939      \n",
       "2018-03-01  kalkaska, mi         -0.355               1.526               -0.223                 0.681                 0.906      \n",
       "2018-02-01  kalkaska, mi         -0.326               1.715               -0.236                 0.898                 0.743      \n",
       "2018-01-01  kalkaska, mi         -0.326               1.564               -0.235                 0.938                 0.318      \n",
       "2017-12-01  kalkaska, mi         -0.326               0.996               -0.225                 1.234                 0.200      \n",
       "2017-11-01  kalkaska, mi         -0.371               0.466               -0.191                 0.938                 0.160      \n",
       "2017-10-01  kalkaska, mi         -0.412               0.201               -0.165                 0.582                 0.122      \n",
       "2017-09-01  kalkaska, mi         -0.446               0.428               -0.139                 0.424                -0.007      \n",
       "2017-08-01  kalkaska, mi         -0.446               0.504               -0.135                 0.087                 0.100      \n",
       "2017-07-01  kalkaska, mi         -0.474               0.845               -0.112                -0.249                 0.225      \n",
       "2017-06-01  kalkaska, mi         -0.474               0.580               -0.099                -0.526                 0.135      \n",
       "2017-05-01  kalkaska, mi         -0.497               0.617               -0.118                -0.545                 0.286      \n",
       "2017-04-01  kalkaska, mi         -0.497               1.110               -0.145                -0.011                 0.336      \n",
       "2017-03-01  kalkaska, mi         -0.497               1.640               -0.160                 1.096                 0.551      \n",
       "2017-02-01  kalkaska, mi         -0.497               1.942               -0.173                 1.175                 0.511      \n",
       "2017-01-01  kalkaska, mi         -0.495               1.867               -0.171                 1.096                 0.486      \n",
       "2016-12-01  kalkaska, mi         -0.495               1.147               -0.132                 0.938                 0.554      \n",
       "2016-11-01  kalkaska, mi         -0.467               0.617               -0.093                 0.938                -0.057      \n",
       "2016-10-01  kalkaska, mi         -0.440               0.504               -0.034                 0.681                -0.485      \n",
       "2016-09-01  kalkaska, mi         -0.440               0.542               -0.018                 0.384                -0.499      \n",
       "2016-08-01  kalkaska, mi         -0.440               0.655               -0.017                 0.167                -0.535      \n",
       "2016-07-01  kalkaska, mi         -0.439               0.920               -0.021                -0.130                -0.528      "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# debugging... why try except no work in this cluster?\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.colheader_justify', 'center')\n",
    "pd.set_option('display.precision', 3)\n",
    "c\n",
    "d3[d3[\"county_name\"]==c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbac03d9-b9c3-4ac1-9f63-f072cf0d0fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_data.y_train.shape\n",
    "r = reshaped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94565378",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28d13821-4ae9-419a-ba5a-928f4e86c037",
   "metadata": {
    "tags": []
   },
   "source": [
    "# FINALLY SOME LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cc8978f-28f1-4990-8ceb-78b8b9de162c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop call\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "\n",
    "class EarlyStopping(Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "    \n",
    "        if(logs.get('mae') < 0.03):\n",
    "          print(\"\\nMAEthreshold reached. Training stopped.\")\n",
    "          self.model.stop_training = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93813c56-5e68-4d97-8763-9b04ff3e4774",
   "metadata": {},
   "source": [
    "## BLSTM dense tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1990278-fe52-46a5-99e8-4f8ace363481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-27 22:58:53.414283: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-27 22:58:53.556701: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-27 22:58:53.556944: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-27 22:58:53.557194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-27 22:58:53.557384: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-27 22:58:53.557569: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-27 22:58:53.557753: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-27 22:58:54.315827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-27 22:58:54.316083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-27 22:58:54.316674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-27 22:58:54.316901: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-27 22:58:54.317094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-27 22:58:54.317275: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9935 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:10:00.0, compute capability: 7.5\n",
      "2022-10-27 22:58:54.317975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-27 22:58:54.318172: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 10709 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:21:00.0, compute capability: 7.5\n",
      "2022-10-27 22:58:57.388623: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 2046736560 exceeds 10% of free system memory.\n",
      "2022-10-27 22:59:00.900943: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 2046736560 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1089/1089 [==============================] - 7s 5ms/step - loss: 0.7042 - mse: 0.7042 - mae: 0.5313 - val_loss: 1.5320 - val_mse: 1.5320 - val_mae: 0.5789\n",
      "Epoch 2/30\n",
      "1089/1089 [==============================] - 5s 4ms/step - loss: 0.6170 - mse: 0.6170 - mae: 0.4673 - val_loss: 1.5011 - val_mse: 1.5011 - val_mae: 0.5809\n",
      "Epoch 3/30\n",
      "1089/1089 [==============================] - 5s 4ms/step - loss: 0.5037 - mse: 0.5037 - mae: 0.3895 - val_loss: 1.5050 - val_mse: 1.5050 - val_mae: 0.5884\n",
      "Epoch 4/30\n",
      "1089/1089 [==============================] - 5s 5ms/step - loss: 0.4555 - mse: 0.4555 - mae: 0.3535 - val_loss: 1.5091 - val_mse: 1.5091 - val_mae: 0.5898\n",
      "Epoch 5/30\n",
      "1089/1089 [==============================] - 5s 4ms/step - loss: 0.4406 - mse: 0.4406 - mae: 0.3431 - val_loss: 1.5086 - val_mse: 1.5086 - val_mae: 0.5900\n",
      "Epoch 6/30\n",
      "1089/1089 [==============================] - 5s 4ms/step - loss: 0.4319 - mse: 0.4319 - mae: 0.3341 - val_loss: 1.5144 - val_mse: 1.5144 - val_mae: 0.5804\n",
      "Epoch 7/30\n",
      "1089/1089 [==============================] - 5s 4ms/step - loss: 0.4275 - mse: 0.4275 - mae: 0.3278 - val_loss: 1.5146 - val_mse: 1.5146 - val_mae: 0.5796\n",
      "Epoch 8/30\n",
      "1089/1089 [==============================] - 5s 4ms/step - loss: 0.4249 - mse: 0.4249 - mae: 0.3226 - val_loss: 1.5193 - val_mse: 1.5193 - val_mae: 0.5731\n",
      "Epoch 9/30\n",
      "1089/1089 [==============================] - 5s 4ms/step - loss: 0.4210 - mse: 0.4210 - mae: 0.3193 - val_loss: 1.5220 - val_mse: 1.5220 - val_mae: 0.5738\n",
      "Epoch 10/30\n",
      "1089/1089 [==============================] - 5s 4ms/step - loss: 0.4162 - mse: 0.4162 - mae: 0.3151 - val_loss: 1.5201 - val_mse: 1.5201 - val_mae: 0.5716\n",
      "Epoch 11/30\n",
      "1089/1089 [==============================] - 5s 5ms/step - loss: 0.4176 - mse: 0.4176 - mae: 0.3156 - val_loss: 1.5156 - val_mse: 1.5156 - val_mae: 0.5717\n",
      "Epoch 12/30\n",
      "1089/1089 [==============================] - 5s 4ms/step - loss: 0.4173 - mse: 0.4173 - mae: 0.3125 - val_loss: 1.5180 - val_mse: 1.5180 - val_mae: 0.5725\n",
      "Epoch 13/30\n",
      "1089/1089 [==============================] - 5s 4ms/step - loss: 0.4164 - mse: 0.4164 - mae: 0.3110 - val_loss: 1.5173 - val_mse: 1.5173 - val_mae: 0.5708\n",
      "Epoch 14/30\n",
      "1089/1089 [==============================] - 5s 4ms/step - loss: 0.4151 - mse: 0.4151 - mae: 0.3107 - val_loss: 1.5190 - val_mse: 1.5190 - val_mae: 0.5707\n",
      "Epoch 15/30\n",
      "1089/1089 [==============================] - 5s 4ms/step - loss: 0.4128 - mse: 0.4128 - mae: 0.3086 - val_loss: 1.5177 - val_mse: 1.5177 - val_mae: 0.5697\n",
      "Epoch 16/30\n",
      "1089/1089 [==============================] - 5s 4ms/step - loss: 0.4107 - mse: 0.4107 - mae: 0.3074 - val_loss: 1.5231 - val_mse: 1.5231 - val_mae: 0.5657\n",
      "Epoch 17/30\n",
      "1089/1089 [==============================] - 5s 4ms/step - loss: 0.4108 - mse: 0.4108 - mae: 0.3065 - val_loss: 1.5238 - val_mse: 1.5238 - val_mae: 0.5665\n",
      "Epoch 18/30\n",
      "1089/1089 [==============================] - 5s 4ms/step - loss: 0.4136 - mse: 0.4136 - mae: 0.3067 - val_loss: 1.5304 - val_mse: 1.5304 - val_mae: 0.5687\n",
      "Epoch 19/30\n",
      "1089/1089 [==============================] - 5s 4ms/step - loss: 0.4088 - mse: 0.4088 - mae: 0.3048 - val_loss: 1.5326 - val_mse: 1.5326 - val_mae: 0.5648\n",
      "Epoch 20/30\n",
      "1089/1089 [==============================] - 5s 4ms/step - loss: 0.4102 - mse: 0.4102 - mae: 0.3051 - val_loss: 1.5314 - val_mse: 1.5314 - val_mae: 0.5670\n",
      "Epoch 21/30\n",
      "1089/1089 [==============================] - 5s 4ms/step - loss: 0.4098 - mse: 0.4098 - mae: 0.3041 - val_loss: 1.5271 - val_mse: 1.5271 - val_mae: 0.5665\n",
      "Epoch 22/30\n",
      "1089/1089 [==============================] - 5s 4ms/step - loss: 0.4090 - mse: 0.4090 - mae: 0.3047 - val_loss: 1.5328 - val_mse: 1.5328 - val_mae: 0.5667\n",
      "Epoch 23/30\n",
      "1089/1089 [==============================] - 5s 4ms/step - loss: 0.4064 - mse: 0.4064 - mae: 0.3025 - val_loss: 1.5431 - val_mse: 1.5431 - val_mae: 0.5646\n",
      "Epoch 24/30\n",
      "1089/1089 [==============================] - 5s 4ms/step - loss: 0.4073 - mse: 0.4073 - mae: 0.3025 - val_loss: 1.5337 - val_mse: 1.5337 - val_mae: 0.5646\n",
      "Epoch 25/30\n",
      "1089/1089 [==============================] - 5s 4ms/step - loss: 0.4061 - mse: 0.4061 - mae: 0.3014 - val_loss: 1.5321 - val_mse: 1.5321 - val_mae: 0.5659\n",
      "Epoch 26/30\n",
      "1089/1089 [==============================] - 5s 4ms/step - loss: 0.4077 - mse: 0.4077 - mae: 0.3018 - val_loss: 1.5362 - val_mse: 1.5362 - val_mae: 0.5649\n",
      "Epoch 27/30\n",
      "1089/1089 [==============================] - 5s 4ms/step - loss: 0.4062 - mse: 0.4062 - mae: 0.3011 - val_loss: 1.5372 - val_mse: 1.5372 - val_mae: 0.5673\n",
      "Epoch 28/30\n",
      "1089/1089 [==============================] - 5s 4ms/step - loss: 0.4096 - mse: 0.4096 - mae: 0.3016 - val_loss: 1.5374 - val_mse: 1.5374 - val_mae: 0.5642\n",
      "Epoch 29/30\n",
      "1089/1089 [==============================] - 5s 4ms/step - loss: 0.4050 - mse: 0.4050 - mae: 0.2999 - val_loss: 1.5452 - val_mse: 1.5452 - val_mae: 0.5666\n",
      "Epoch 30/30\n",
      "1089/1089 [==============================] - 5s 4ms/step - loss: 0.4070 - mse: 0.4070 - mae: 0.3000 - val_loss: 1.5431 - val_mse: 1.5431 - val_mae: 0.5683\n"
     ]
    }
   ],
   "source": [
    "# try bidirectional\n",
    "#https://analyticsindiamag.com/complete-guide-to-bidirectional-lstm-with-python-codes/\n",
    "# https://keras.io/api/layers/recurrent_layers/bidirectional/\n",
    "# https://www.google.com/search?q=why+only+one+layer+is+enough+lstm&oq=why+only+one+layer+is+enough+lstm&aqs=chrome..69i57j33i160l3j33i299l3.3544j0j1&sourceid=chrome&ie=UTF-8\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dropout, Dense, Bidirectional\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "blstm_model = Sequential()\n",
    "blstm_model.reset_states()\n",
    "blstm_model.add(Bidirectional(LSTM(64, activation='relu', dropout=0.5, recurrent_activation='tanh', input_shape=(r.x_train.shape[1], r.x_train.shape[2]), return_sequences=False)))\n",
    "\n",
    "# model.add(Dropout(0.09))\n",
    "blstm_model.add(Dense(1, activation='tanh'))\n",
    "\n",
    "blstm_model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse','mae'])\n",
    "# https://medium.com/@canerkilinc/selecting-optimal-lstm-batch-size-63066d88b96b#:~:text=By%20experience%2C%20in%20most%20cases,based%20on%20the%20performance%20observation. \n",
    "# optimal batch size = 60\n",
    "blstm = blstm_model.fit(r.x_train, r.y_train, epochs=30, batch_size=160, validation_split=0.1,  verbose=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6db97db-572d-4330-8aaf-cf1ce67839d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# , this is with heavy dropout in lstm\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(blstm.history['loss'], label='Loss')\n",
    "plt.plot(blstm.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bcccd2-8084-436e-bf2d-ca5c6208075f",
   "metadata": {},
   "source": [
    "### BLSTM batch_size=64\n",
    "### 100 units, 20 epochs, solo dropout=0.2, loss=mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d18ccb-d75e-4db5-8e8f-9977675f27ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "# try bidirectional\n",
    "#https://analyticsindiamag.com/complete-guide-to-bidirectional-lstm-with-python-codes/\n",
    "# https://keras.io/api/layers/recurrent_layers/bidirectional/\n",
    "# https://www.google.com/search?q=why+only+one+layer+is+enough+lstm&oq=why+only+one+layer+is+enough+lstm&aqs=chrome..69i57j33i160l3j33i299l3.3544j0j1&sourceid=chrome&ie=UTF-8\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dropout, Dense, Bidirectional\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping()\n",
    "\n",
    "blstm_model.reset_states()\n",
    "\n",
    "\n",
    "blstm_model = Sequential()\n",
    "blstm_model.add(Bidirectional(LSTM(100, activation='relu', recurrent_activation='tanh', input_shape=(r.x_train.shape[1], r.x_train.shape[2]), return_sequences=False)))\n",
    "blstm_model.add(Dropout(0.2))\n",
    "\n",
    "# model.add(Dropout(0.09))\n",
    "blstm_model.add(Dense(1, activation='linear'))\n",
    "\n",
    "blstm_model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "# https://medium.com/@canerkilinc/selecting-optimal-lstm-batch-size-63066d88b96b#:~:text=By%20experience%2C%20in%20most%20cases,based%20on%20the%20performance%20observation. \n",
    "# optimal batch size = 60\n",
    "blstm = blstm_model.fit(r.x_train, r.y_train, epochs=20, batch_size=64, validation_split=0.1,  verbose=1, shuffle=False, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc518b02-a38f-465d-8b45-cde22bbaf4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(blstm.history['loss'], label='Loss')\n",
    "plt.plot(blstm.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873dc568-4ba2-4064-8c1a-7e2bd0c176e1",
   "metadata": {},
   "source": [
    "### BLSTM batch_size=64\n",
    "### 30 units, 20 epochs, no dropout=0.2, loss=mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138da7ee-2d17-4f1b-8da8-341709b6117e",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b90c67f-8a53-47e2-b0fd-ec3e0569c0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# try bidirectional\n",
    "#https://analyticsindiamag.com/complete-guide-to-bidirectional-lstm-with-python-codes/\n",
    "# https://keras.io/api/layers/recurrent_layers/bidirectional/\n",
    "# https://www.google.com/search?q=why+only+one+layer+is+enough+lstm&oq=why+only+one+layer+is+enough+lstm&aqs=chrome..69i57j33i160l3j33i299l3.3544j0j1&sourceid=chrome&ie=UTF-8\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dropout, Dense, Bidirectional\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "blstm_model.reset_states()\n",
    "blstm_model = Sequential()\n",
    "blstm_model.add(Bidirectional(LSTM(30, activation='relu', recurrent_activation='tanh', input_shape=(r.x_train.shape[1], r.x_train.shape[2]), return_sequences=False)))\n",
    "\n",
    "\n",
    "# model.add(Dropout(0.09))\n",
    "blstm_model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Huber is less prone to outliers aka COVID 19 BABY\n",
    "# https://en.wikipedia.org/wiki/Huber_loss\n",
    "blstm_model.compile(loss=tf.keras.losses.Huber(), optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=['mae'])\n",
    "# https://medium.com/@canerkilinc/selecting-optimal-lstm-batch-size-63066d88b96b#:~:text=By%20experience%2C%20in%20most%20cases,based%20on%20the%20performance%20observation. \n",
    "# optimal batch size = 64\n",
    "blstm = blstm_model.fit(r.x_train, r.y_train, epochs=20, batch_size=64, validation_split=0.1,  verbose=1, shuffle=False, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a29258-5eb5-4e16-bd58-243f3ea70928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot for ^\n",
    "plt.plot(blstm.history['loss'], label='Huber Loss')\n",
    "plt.plot(blstm.history['mae'], label='mae')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1487015f-9927-4faf-a11e-9acc2743a19c",
   "metadata": {},
   "source": [
    "## BLSTM HELLA LAYERS AND HELLA UNITS PER LAYER BRO\n",
    "### no recurrent activation no activation in any layers \n",
    "(saw this on kaggle) more doesn't mean better. We will go with one above. We will continue with that model below.\n",
    "https://towardsdatascience.com/time-series-prediction-with-lstm-in-tensorflow-42104db39340 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bc7932-d447-43f5-a1ec-a1849bb158ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# try bidirectional\n",
    "#https://analyticsindiamag.com/complete-guide-to-bidirectional-lstm-with-python-codes/\n",
    "# https://keras.io/api/layers/recurrent_layers/bidirectional/\n",
    "# https://www.google.com/search?q=why+only+one+layer+is+enough+lstm&oq=why+only+one+layer+is+enough+lstm&aqs=chrome..69i57j33i160l3j33i299l3.3544j0j1&sourceid=chrome&ie=UTF-8\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dropout, Dense, Bidirectional, Lambda\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "blstm_model.reset_states()\n",
    "blstm_model = Sequential()\n",
    "\n",
    "# blstm_model.add(Lambda(lambda x: tf.expand_dims(x, axis=-1), input_shape=[None]))\n",
    "blstm_model.add(Bidirectional(LSTM(1024, input_shape=(r.x_train.shape[1], r.x_train.shape[2]), return_sequences=True)))\n",
    "blstm_model.add(Bidirectional(LSTM(512, input_shape=(r.x_train.shape[1], r.x_train.shape[2]), return_sequences=True)))\n",
    "blstm_model.add(Bidirectional(LSTM(256, input_shape=(r.x_train.shape[1], r.x_train.shape[2]), return_sequences=True)))\n",
    "blstm_model.add(Bidirectional(LSTM(128, input_shape=(r.x_train.shape[1], r.x_train.shape[2]), return_sequences=True)))\n",
    "blstm_model.add(Bidirectional(LSTM(64, input_shape=(r.x_train.shape[1], r.x_train.shape[2]), return_sequences=False)))\n",
    "\n",
    "# model.add(Dropout(0.09))\n",
    "blstm_model.add(Dense(1))\n",
    "\n",
    "# Huber is less prone to outliers aka COVID 19 BABY\n",
    "# https://en.wikipedia.org/wiki/Huber_loss\n",
    "blstm_model.compile(loss=tf.keras.losses.Huber(), optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=['mae'])\n",
    "# https://medium.com/@canerkilinc/selecting-optimal-lstm-batch-size-63066d88b96b#:~:text=By%20experience%2C%20in%20most%20cases,based%20on%20the%20performance%20observation. \n",
    "# optimal batch size = 64\n",
    "blstm = blstm_model.fit(r.x_train, r.y_train, epochs=20, batch_size=64, validation_split=0.1,  verbose=1, shuffle=False, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ec3132-ff9e-42b4-bae3-6233c69b402c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(blstm.history['loss'], label='Huber Loss')\n",
    "plt.plot(blstm.history['mae'], label='mae')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd7d04d-2bc2-46b7-82c8-708905ce053f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# try bidirectional\n",
    "#https://analyticsindiamag.com/complete-guide-to-bidirectional-lstm-with-python-codes/\n",
    "# https://keras.io/api/layers/recurrent_layers/bidirectional/\n",
    "# https://www.google.com/search?q=why+only+one+layer+is+enough+lstm&oq=why+only+one+layer+is+enough+lstm&aqs=chrome..69i57j33i160l3j33i299l3.3544j0j1&sourceid=chrome&ie=UTF-8\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dropout, Dense, Bidirectional\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "blstm_model.reset_states()\n",
    "blstm_model = Sequential()\n",
    "# recurrent: activate input/forget/output gate\n",
    "# activation: cell state and hidden state\n",
    "blstm_model.add(Bidirectional(LSTM(30, activation='relu', recurrent_activation='tanh', input_shape=(r.x_train.shape[1], r.x_train.shape[2]), return_sequences=False)))\n",
    "\n",
    "# https://datascience.stackexchange.com/questions/38205/dropout-on-which-layers-of-lstm#:~:text=The%20logic%20of%20drop%20out,layers%20after%20the%20LSTM%20layers.\n",
    "# says dropout layer should not be added due to lstm's affinity for not remembering many things at once\n",
    "# chance of forgetting something that should not be forgotten\n",
    "# model.add(Dropout(0.09))\n",
    "blstm_model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Huber is less prone to outliers \n",
    "# aka COVID 19 BABY\n",
    "# https://en.wikipedia.org/wiki/Huber_loss\n",
    "blstm_model.compile(loss=tf.keras.losses.Huber(), optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=['mae'])\n",
    "# https://medium.com/@canerkilinc/selecting-optimal-lstm-batch-size-63066d88b96b#:~:text=By%20experience%2C%20in%20most%20cases,based%20on%20the%20performance%20observation. \n",
    "# optimal batch size = 64\n",
    "blstm = blstm_model.fit(r.x_train, r.y_train, epochs=20, batch_size=64, validation_split=0.1,  verbose=1, shuffle=False, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1e2b43-1df7-4b6f-a29c-1111dcbb49c3",
   "metadata": {},
   "source": [
    "## BLSTM DENSE LINEAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54f73ae-d983-4ae0-95db-230248833f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try bidirectional\n",
    "#https://analyticsindiamag.com/complete-guide-to-bidirectional-lstm-with-python-codes/\n",
    "# https://keras.io/api/layers/recurrent_layers/bidirectional/\n",
    "# https://www.google.com/search?q=why+only+one+layer+is+enough+lstm&oq=why+only+one+layer+is+enough+lstm&aqs=chrome..69i57j33i160l3j33i299l3.3544j0j1&sourceid=chrome&ie=UTF-8\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dropout, Dense, Bidirectional\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "blstm_model = Sequential()\n",
    "blstm_model.reset_states()\n",
    "blstm_model.add(Bidirectional(LSTM(64, activation='relu', dropout=0.09, recurrent_activation='tanh', input_shape=(r.x_train.shape[1], r.x_train.shape[2]), return_sequences=False)))\n",
    "# model.add(LSTM(64, activation='sigmoid', return_sequences=True))\n",
    "# model.add(LSTM(128, activation='sigmoid', return_sequences=True))\n",
    "# model.add(LSTM(64, activation='sigmoid', return_sequences=True))\n",
    "# model.add(LSTM(32, activation='sigmoid', return_sequences=False))\n",
    "\n",
    "# model.add(Dropout(0.09))\n",
    "blstm_model.add(Dense(1, activation='linear'))\n",
    "\n",
    "blstm_model.compile(loss=tf.keras.losses.Huber(), optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics=['mae'])\n",
    "blstm = blstm_model.fit(r.x_train, r.y_train, epochs=30, batch_size=64, validation_split=0.1,  verbose=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd73306-40c2-486d-9932-d09135650093",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a7c2b2-f40a-45cd-8966-d0e6880ae58c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2020a331-8648-49f8-aeed-1aaf93e02284",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_array_steps(array_steps_df, time_steps, d.columns, 'county_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3002511c-983e-43e5-b749-fb7f6b533468",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_array_steps(array_steps_df, time_steps, columns_to_array, id_column):\n",
    "    \"\"\"\n",
    "    https: //www.mikulskibartosz.name/ how - to - turn - pandas - data - frame - into - time - series - input - for -rnn /\n",
    "    :param array_steps_df: the dataframe from the csv\n",
    "    :param time_steps: how many time steps\n",
    "    :param columns_to_array: what columns to convert to the array\n",
    "    :param id_column: what is to be used for the identifier\n",
    "    :return: data grouped in a # observations by identifier and date\n",
    "    \"\"\"\n",
    "\n",
    "    id_list = array_steps_df[id_column].unique().tolist()\n",
    "    date_list = array_steps_df['date'].unique().tolist()\n",
    "\n",
    "    master_list = []\n",
    "    target_list = []\n",
    "\n",
    "    missing_counter = 0\n",
    "    total_counter = 0\n",
    "\n",
    "    # grab date size = time steps at a time and iterate through all of them\n",
    "    for date in range(len(date_list) - time_steps + 1):\n",
    "        date_range_test = date_list[date:time_steps+date]\n",
    "\n",
    "        date_range_df = array_steps_df.loc[(array_steps_df['date'] <= date_range_test[-1]) &\n",
    "                                           (array_steps_df['date'] >= date_range_test[0])\n",
    "                                          ]\n",
    "\n",
    "        # for each id do it separately so time series data doesn't get mixed up\n",
    "        for identifier in id_list:\n",
    "\n",
    "            # get id in here and then skip if not the required time steps/observations for the id\n",
    "\n",
    "            date_range_id = date_range_df.loc[date_range_df[id_column] == identifier]\n",
    "\n",
    "            master_dict = {}\n",
    "\n",
    "        # if there aren't enough observations for the data range\n",
    "            if len(date_range_id) != time_steps:\n",
    "\n",
    "                # dont fully need the counter except in unusual circumstances when debugging it causes no harm for now\n",
    "                missing_counter += 1\n",
    "\n",
    "            else:\n",
    "            # add target each loop through for the last date in the date range for the id or ticker\n",
    "                target = array_steps_df['target'].\\\n",
    "                         loc[(array_steps_df['date'] == date_range_test[-1])\n",
    "                           & (array_steps_df[id_column] == identifier)                                     \n",
    "                            ].iloc[0]\n",
    "\n",
    "                target_list.append(target)\n",
    "\n",
    "                total_counter += 1\n",
    "\n",
    "                # loop through each column in dataframe\n",
    "                for column in columns_to_array:\n",
    "\n",
    "                    date_range_id_value = date_range_id[[column]].values\n",
    "\n",
    "                    master_dict[column] = []\n",
    "                    master_dict[column].append(date_range_id_value)\n",
    "\n",
    "                master_list.append(master_dict)\n",
    "\n",
    "    # redo columns to arrays, after they have been ordered and grouped by Id above\n",
    "    array_list = []\n",
    "\n",
    "    # for each column go through the values in the array create an array for the column then append to list\n",
    "    for column in columns_to_array:\n",
    "\n",
    "        for idx, dic in enumerate(master_list):\n",
    "            # init arrays here if the first value\n",
    "            if idx == 0:\n",
    "                 value_array_init = master_list[0][column]\n",
    "\n",
    "            else:\n",
    "                 value_array_init += master_list[idx][column]\n",
    "\n",
    "        array_list.append(np.array(value_array_init))\n",
    "\n",
    "    # for each value in the array list, horizontally stack each value\n",
    "    all_array = np.hstack(array_list).reshape((total_counter,\n",
    "                                               len(columns_to_array),\n",
    "                                               time_steps\n",
    "                                               )\n",
    "                                             ).transpose(0, 2, 1)\n",
    "\n",
    "    target_array_all = np.array(target_list\n",
    "                                ).reshape(len(target_list),\n",
    "                                          1)\n",
    "\n",
    "    # should probably make this an if condition later after a few more tests\n",
    "    print('check of length of arrays', len(all_array), len(target_array_all))\n",
    "\n",
    "    return all_array, target_array_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb5cb29-1695-4554-a498-4929524dd1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tried embeddiung entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cf469c-21a0-4cad-9e31-bea5a3cb9ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cont, cat = cont_cat_split(d, max_card=24, dep_var='median_listing_price')\n",
    "cont, cat\n",
    "procs_nn = [Categorify]\n",
    "splits = RandomSplitter(seed=23)(d)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "to = TabularPandas(d, procs_nn, cat, cont,\n",
    "                      splits=splits, y_names='median_listing_price')\n",
    "dls = to_nn.dataloaders(1024, device = device)\n",
    "learn = tabular_learner(dls, layers=[500,250], n_out=1)\n",
    "learn.fit_one_cycle(8, 5e-4)\n",
    "def embed_features(learner, xs):\n",
    "  \"\"\"\n",
    "  learner: fastai Learner used to train the neural net\n",
    "  xs: DataFrame containing input variables with nominal values defined by their rank.\n",
    "  ::returns:: a copy of `xs` with embeddings replacing each categorical variable\n",
    "  \"\"\"\n",
    "  xs = xs.copy()\n",
    "  for i,col in enumerate(learn.dls.cat_names):\n",
    "    emb = learn.model.embeds[i]\n",
    "    emb_data = emb(tensor(xs[col], dtype=torch.int64).to(device))\n",
    "    emb_names = [f'{col}_{j}' for j in range(emb_data.shape[1])]\n",
    "    feat_df = pd.DataFrame(data=emb_data, index=xs.index, columns=emb_names)\n",
    "    xs = xs.drop(col, axis=1)\n",
    "    xs = xs.join(feat_df)\n",
    "  return xs\n",
    "emb_xs = embed_features(learn, to.train.xs)\n",
    "emb_valid_xs = embed_features(learn, to.valid.xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a70021-c9ab-419b-9b36-aa4f805859d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = d[d.date<\"2021-07-01\"]\n",
    "d_test = d[d.date>=\"2021-04-01\"]\n",
    "\n",
    "# train\n",
    "x_train = d_train.loc[:, d_train.columns != 'median_listing_price']\n",
    "x_test = d_test.loc[:, d_test.columns != 'median_listing_price']\n",
    "# test\n",
    "y_train = d_train['median_listing_price']\n",
    "y_test = d_test['median_listing_price']\n",
    "# var split\n",
    "cat_vars = ['county_name']\n",
    "cont = list(x_train.columns)\n",
    "cont.remove('date')\n",
    "cont.remove('county_name')\n",
    "\n",
    "D_train = []\n",
    "D_test = []\n",
    "D_train.append(x_train[cont].astype('float64').values)\n",
    "D_train.append(x_train[cat_vars].values)\n",
    "D_test.append(x_test[cat_vars].values)\n",
    "D_test.append(x_test[cont].astype('float64').values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce8560e-e764-4677-9d81-fda4444cf759",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_sizes = {}\n",
    "cat_embsizes = {}\n",
    "for cat in cat_vars:\n",
    "    cat_sizes[cat] = train_data[cat].nunique()\n",
    "    cat_embsizes[cat] = min(50, cat_sizes[cat]//2+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0f3c2e-dc53-4c10-a9c8-68580236637d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout, Embedding, Input, Reshape, Concatenate\n",
    "from keras.models import Model\n",
    "import keras.backend as K\n",
    "\n",
    "def custom_smape(x, x_):\n",
    "    return K.mean(2*K.abs(x-x_)/(K.abs(x)+K.abs(x_)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c90cfb-703c-409e-860c-997405e6573d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ins = []\n",
    "concat = []\n",
    "for cat in cat_vars:\n",
    "    x = Input((1,), name=cat)\n",
    "    ins.append(x)\n",
    "    x = Embedding(cat_sizes[cat]+1, cat_embsizes[cat], input_length=1)(x)\n",
    "    x = Reshape((cat_embsizes[cat],))(x)\n",
    "    concat.append(x)\n",
    "\n",
    "y = Input((len(cont),), name='cont_vars')\n",
    "ins.append(y)\n",
    "concat.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba341a8-1596-4d64-8e11-f373343c7496",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = Concatenate()(concat)\n",
    "y = Dense(100, activation= 'relu')(y)\n",
    "y = Dense(1)(y)\n",
    "model = Model(ins, y)\n",
    "model.compile('adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d06e7a-d06c-4055-ad4a-64c76c7d000e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca62700b-3948-464b-897f-5c7266dab81f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee45892d-1751-4729-b513-784daf22dbdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da486ded-0de0-4504-af26-2d76ede66e7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf2f30a-c110-4ab9-8feb-5042b7b796c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try sliding window panel\n",
    "# https://stackoverflow.com/questions/40008240/how-to-process-panel-data-for-use-in-a-recurrent-neural-network-rnn\n",
    "# https://timeseriesai.github.io/tsai/data.preparation.html#SlidingWindowPanel\n",
    "\n",
    "from tsai.data.preparation import SlidingWindowPanel\n",
    "# d2\n",
    "d2_train = d2[d2['date']<='2022-05-01']\n",
    "d2_test = d2[d2['date']>'2022-05-01']\n",
    "\n",
    "xvars = ['unemployment_rate','active_listing_count','median_days_on_market', 'median_listing_price']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "enc = LabelEncoder()\n",
    "train_data = pd.DataFrame()\n",
    "train_data[d2.columns] = d2\n",
    "\n",
    "train_columns_to_scale = list(train_data.columns)\n",
    "train_data[xvars] = scaler.fit_transform(train_data[xvars])\n",
    "train_data['county_name'] = enc.fit_transform(train_data['county_name'])\n",
    "train_data.set_index('date',inplace=True)\n",
    "\n",
    "d2_Xtrain, d2_ytrain = X, y = SlidingWindowPanel(window_len=5, unique_id_cols=['county_name'], stride=1, start=0, get_x=xvars.remove('median_listing_price')\n",
    "                                                 , get_y=['median_listing_price'], horizon=1, seq_first=True, sort_by=['date'], ascending=True, return_key=False)(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9435e88f-cef5-45ee-a3e7-469863e82bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2_Xtrain.shape, d2_ytrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9fef7d-629e-4613-974d-b0599a25c3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try bidirectional\n",
    "#https://analyticsindiamag.com/complete-guide-to-bidirectional-lstm-with-python-codes/\n",
    "# https://keras.io/api/layers/recurrent_layers/bidirectional/\n",
    "# https://www.google.com/search?q=why+only+one+layer+is+enough+lstm&oq=why+only+one+layer+is+enough+lstm&aqs=chrome..69i57j33i160l3j33i299l3.3544j0j1&sourceid=chrome&ie=UTF-8\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dropout, Dense, Bidirectional, TimeDistributed\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "blstm_model = Sequential()\n",
    "blstm_model.reset_states()\n",
    "blstm_model.add(Bidirectional(LSTM(64, activation='sigmoid', dropout=0.09, recurrent_activation='tanh', input_shape=(74, d2_X.shape[2]), return_sequences=False)))\n",
    "# model.add(LSTM(64, activation='sigmoid', return_sequences=True))\n",
    "# model.add(LSTM(128, activation='sigmoid', return_sequences=True))\n",
    "# model.add(LSTM(64, activation='sigmoid', return_sequences=True))\n",
    "# model.add(LSTM(32, activation='sigmoid', return_sequences=False))\n",
    "\n",
    "#https://stackoverflow.com/questions/60344253/lstm-model-has-constant-accuracy-and-doesnt-variate\n",
    "\n",
    "blstm_model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# https://stackoverflow.com/questions/45632549/why-is-the-accuracy-for-my-keras-model-always-0-when-training\n",
    "\n",
    "blstm_model.compile(loss='mse', optimizer='adam')# , metrics=['accuracy'])\n",
    "blstm = blstm_model.fit(d2_Xtrain, d2_ytrain, epochs=30, batch_size=160, validation_split=0.1,  verbose=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81000c27-2fad-4a8f-926f-385bb53fe808",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(blstm.history['loss'], label='Loss')\n",
    "plt.plot(blstm.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "ypred=blstm_model.predict(rdat.x_test)\n",
    "# check length\n",
    "ypred.shape,rdat.y_test.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_engineering_venv",
   "language": "python",
   "name": "data_engineering_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
